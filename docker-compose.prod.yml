version: "3.9"

services:
  db:
    image: postgres:14
    container_name: securechat_db
    restart: always
    env_file: .env.prod
    volumes:
      - db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 6
      start_period: 10s
    labels:
      - autoheal=true  # Autoheal peut redémarrer la DB si unhealthy

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: securechat_backend
    restart: always
    depends_on:
      db:
        condition: service_healthy
    env_file: .env.prod
    # en prod on n'a plus besoin des variables Flask
    volumes:
      # migrations en lecture/écriture
      - ./backend/migrations:/app/migrations
      # média / fichiers statiques générés par FastAPI (selon ton settings.MEDIA_ROOT)
      - ./backend/media:/app/media
    # on écoute en local, le Nginx de la machine fera le reverse-proxy vers 8000
    ports:
      - "127.0.0.1:8000:8000"
    entrypoint: >
      sh -c "
        /wait-for-it.sh db:5432 --timeout=60 --strict &&
        alembic -c /app/alembic.ini upgrade head &&
        gunicorn 'app.main:app' -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000 --workers=4 --threads=2 --timeout=120
      "
    healthcheck:
      # on teste le /healthz que tu as dans app.main
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8000/healthz >/dev/null || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 25s
    labels:
      - autoheal=true

  worker:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: securechat_worker
    restart: always
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_started
    env_file: .env.prod
    volumes:
      - ./backend:/app
    entrypoint: >
      sh -c "
        /wait-for-it.sh db:5432 --timeout=60 --strict &&
        python -m app.workers.notification_worker
      "
    labels:
      - autoheal=true

  # si tu veux garder Redis aussi en prod :
  redis:
    image: redis:7-alpine
    container_name: securechat_redis
    command: ["redis-server", "--appendonly", "yes"]
    volumes:
      - redis_data:/data
    restart: always
    labels:
      - autoheal=true

  frontend:
    build:
      context: ./frontend/secure_messagerie
      dockerfile: Dockerfile.prod   # image nginx qui sert le build
    container_name: securechat_frontend
    restart: always
    depends_on:
      backend:
        condition: service_healthy
    # Nginx de ta machine reverse-proxy vers 8080
    ports:
      - "127.0.0.1:8080:80"
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost/ || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 20s
    labels:
      - autoheal=true

  uptime_kuma:
    image: louislam/uptime-kuma:latest
    container_name: uptime_kuma
    restart: always
    environment:
      - BASE_PATH=/monitoring
    ports:
      - "127.0.0.1:3001:3001"
    volumes:
      - /srv/uptime-kuma:/app/data
    labels:
      - autoheal=true

  autoheal:
    image: willfarrell/autoheal:latest
    container_name: autoheal
    restart: always
    environment:
      # on veut qu'il soigne les conteneurs ayant le label autoheal=true
      - AUTOHEAL_CONTAINER_LABEL=autoheal
      - AUTOHEAL_INTERVAL=20
      - CURL_TIMEOUT=10
      - DOCKER_SOCK=/var/run/docker.sock
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock

volumes:
  db_data:
  redis_data:
